MONTH(logdate),
DAY(logdate),
set_time,
s.l_activity_id,
eez_code,
hooks_n
order by vi.flag_id, vi.vesselname,	YEAR(logdate),  MONTH(logdate),  DAY(logdate) ")
table(ls_dat$eez_code)
save(ls_dat, file = paste0(dir_pth, "Data/OPLL_dat_five_eezs.RData"))
library(ggplot2)
library(data.table)
library(scales)
library(reshape2)
library(magrittr)
library(dplyr)
library(xtable)
library(tidyr)
library(stringr)
library(grid)
library(lubridate)
library(RColorBrewer)
#library(ggmap)
library(maps)
library(mapproj)
library(colorspace)
theme_set(theme_bw())
dir_pth <- "C:/NZ_Seabirds_2025/"
# Prepare a base map for later plotting
mp1 <- fortify(map(fill=TRUE, plot=FALSE))
mp2 <- mp1
mp2$long <- mp2$long + 360
mp2$group <- mp2$group + max(mp2$group) + 1
mp <- rbind(mp1, mp2)
eez <- read.csv(paste0(dir_pth, "Data/EEZ_2016.csv"), header=T)
dir_pth <- "C:/NZseabirds_2025/"
eez <- read.csv(paste0(dir_pth, "Data/EEZ_2016.csv"), header=T)
dir_pth <- "C:/Seabirds/NZseabirds_2025/"
eez <- read.csv(paste0(dir_pth, "Data/EEZ_2016.csv"), header=T)
eez$group <- 1
windows(2000,1400)
pl = ggplot(data = mp, aes(x=long, y=lat, group=group)) +
scale_x_continuous(limits=c(140, 235)) + # + ggtitle(paste(flags, plotname, sep="   "))
scale_y_continuous(limits=c(-40, -10)) + coord_equal() +
#geom_polygon(bnds, mapping=aes(x=x, y=y, group=group), fill=alpha("black", 0.1)) +
#geom_polygon(bnds1, mapping=aes(x=x, y=y, group=group), fill=alpha("red", 0.1)) +
#geom_point(data = pl.dat, aes(x = lond, y = latd, group = group, size = Effort), colour=alpha("red", 0.5)) +
#                                                  colour = Fleet), position=position_jitter(width=2,height=2), alpha = 0.7) +
#scale_size_continuous(range = c(3, 10)) +
xlab('Longitude') + ylab('Latitude') + geom_path() +
geom_path(data=eez, aes(x=POINT_X, y=POINT_Y, group=group), colour="blue") + geom_hline(yintercept=-25, colour="red")
theme(panel.grid.major=element_blank(),
panel.grid.minor=element_blank(),
plot.margin=unit(c(0.2,0.2,0.2,0.2), "cm"))
print(pl)
windows(2000,1400)
pl = ggplot(data = mp, aes(x=long, y=lat, group=group)) +
scale_x_continuous(limits=c(140, 235)) + # + ggtitle(paste(flags, plotname, sep="   "))
scale_y_continuous(limits=c(-40, -10)) + coord_equal() +
#geom_polygon(bnds, mapping=aes(x=x, y=y, group=group), fill=alpha("black", 0.1)) +
#geom_polygon(bnds1, mapping=aes(x=x, y=y, group=group), fill=alpha("red", 0.1)) +
#geom_point(data = pl.dat, aes(x = lond, y = latd, group = group, size = Effort), colour=alpha("red", 0.5)) +
#                                                  colour = Fleet), position=position_jitter(width=2,height=2), alpha = 0.7) +
#scale_size_continuous(range = c(3, 10)) +
xlab('Longitude') + ylab('Latitude') + geom_path() +
geom_path(data=eez, aes(x=POINT_X, y=POINT_Y, group=group), colour="blue") + geom_hline(yintercept=-25, colour="red")
theme(panel.grid.major=element_blank(),
panel.grid.minor=element_blank(),
plot.margin=unit(c(0.2,0.2,0.2,0.2), "cm"))
print(pl)
savePlot(file=paste0(dir_pth, "/Plots/Regional_EEZ_map.png"), type = "png")
dev.off()
load(paste0(dir_pth, "/Data/OPLL_dat_five_eezs.RData"))
load(paste0(dir_pth, "/Data/OPLL_dat_five_eezs.RData"), verbose = TRUE)
dat <- ls_dat
head(dat)
dir_pth <- "C:/Seabirds/NZseabirds_2025/"
db1 <- odbcDriverConnect(connection="driver=SQL Server;server=NOUFAMESQL04; Database=FISH_MASTER;
Trusted_Connection=yes;")
fm_tabs <- sqlTables(db1)
trip_cols <- sqlColumns(db1, "log.trips_ll")
set_cols <- sqlColumns(db1, "log.sets_ll")
catch_cols <- sqlColumns(db1, "log.catch_ll")
ves_cols <- sqlColumns(db1, "ref.vessels")
ves_inst_cols <- sqlColumns(db1, "ref.vessel_instances")
# All logsheets for a certain EEZ
ls_dat <- sqlQuery(db1, "SELECT
t.log_trip_id,
s.log_set_id,
REPLACE(vi.vesselname,'''',' ') as vesselname,
v.vessel_id,
vi.flag_id,
v.loa,
v.grt,
s.latd,
s.lond,
s.in_AWs as in_arch,
YEAR(logdate) as YY,
MONTH(logdate) as MM,
DAY(logdate) as DD,
COALESCE(set_time,'0') as set_start,
s.l_activity_id as l_act_id,
eez_code,
hooks_n,
isnull(cast(sum(case when sp_code = 'ALB' then coalesce(c.sp_kg,00000) else 00000 end) as int),0) as ALB,
ISNULL(cast(sum(case when sp_code = 'YFT' then coalesce(c.sp_kg,00000) else 00000 end) as int),0) as YFT,
ISNULL(cast(sum(case when sp_code = 'BET' then coalesce(c.sp_kg,00000) else 00000 end) as int),0) as BET,
ISNULL(cast(sum(case when sp_code = 'SKJ' then coalesce(c.sp_kg,00000) else 00000 end) as int),0) as SKJ,
ISNULL(cast(sum(case when sp_code = 'PBF' then coalesce(c.sp_kg,00000) else 00000 end) as int),0) as PBF,
ISNULL(cast(sum(case when sp_code = 'SBF' then coalesce(c.sp_kg,00000) else 00000 end) as int),0) as SBF,
ISNULL(cast(sum(case when sp_code = 'BLM' then coalesce(c.sp_kg,00000) else 00000 end) as int),0) as BLM,
ISNULL(cast(sum(case when sp_code = 'BUM' then coalesce(c.sp_kg,00000) else 00000 end) as int),0) as BUM,
ISNULL(cast(sum(case when sp_code = 'MLS' then coalesce(c.sp_kg,00000) else 00000 end) as int),0) as MLS,
ISNULL(cast(sum(case when sp_code = 'SFA' then coalesce(c.sp_kg,00000) else 00000 end) as int),0) as SFA,
ISNULL(cast(sum(case when sp_code = 'SSP' then coalesce(c.sp_kg,00000) else 00000 end) as int),0) as SSP,
ISNULL(cast(sum(case when sp_code = 'SWO' then coalesce(c.sp_kg,00000) else 00000 end) as int),0) as SWO,
ISNULL(cast(sum(case when sp_code = 'BSH' then coalesce(c.sp_kg,00000) else 00000 end) as int),0) as BSH,
ISNULL(cast(sum(case when sp_code = 'BSH' then coalesce(c.spdiscard_kg,00000) else 00000 end) as int),0) as BSH_d,
ISNULL(cast(sum(case when sp_code = 'FAL' then coalesce(c.sp_kg,00000) else 00000 end) as int),0) as FAL,
ISNULL(cast(sum(case when sp_code = 'FAL' then coalesce(c.spdiscard_kg,00000) else 00000 end) as int),0) as FAL_d,
ISNULL(cast(sum(case when sp_code in ('MAK','SMA','LMA') then coalesce(c.sp_kg,00000) else 00000 end) as int),0) as MAK,
ISNULL(cast(sum(case when sp_code in ('MAK','SMA','LMA') then coalesce(c.spdiscard_kg,00000) else 00000 end) as int),0) as MAK_d,
ISNULL(cast(sum(case when sp_code = 'OCS' then coalesce(c.sp_kg,00000) else 00000 end) as int),0) as OCS,
ISNULL(cast(sum(case when sp_code = 'OCS' then coalesce(c.spdiscard_kg,00000) else 00000 end) as int),0) as OCS_d,
ISNULL(cast(sum(case when sp_code in ('THR','PLS','BTH','ALV') then coalesce(c.sp_kg,00000) else 00000 end) as int),0) as THR,
ISNULL(cast(sum(case when sp_code in ('THR','PLS','BTH','ALV') then coalesce(c.spdiscard_kg,00000) else 00000 end) as int),0) as THR_d,
ISNULL(cast(sum(case when sp_code = 'SHK' then coalesce(c.sp_kg,00000) else 00000 end) as int),0) as SHK,
ISNULL(cast(sum(case when sp_code = 'SHK' then coalesce(c.spdiscard_kg,00000) else 00000 end) as int),0) as SHK_d,
ISNULL(cast(sum(case when sp_code NOT in ('ALB','YFT','BET','SKJ','PBF','SBF','BLM','BUM','MLS','SFA','SSP','SWO','BSH','FAL','MAK','OCS','THR','SHK','SMA','LMA','BTH','PTH','ALV') then coalesce(c.sp_kg,00000) else 00000 end) as int),0) as NON,
ISNULL(cast(sum(case when sp_code NOT in ('ALB','YFT','BET','SKJ','PBF','SBF','BLM','BUM','MLS','SFA','SSP','SWO','BSH','FAL','MAK','OCS','THR','SHK','SMA','LMA','BTH','PTH','ALV') then coalesce(c.spdiscard_kg,00000) else 00000 end) as int),0) as NON_d
FROM log.trips_ll t
INNER JOIN log.sets_ll s on t.log_trip_id = s.log_trip_id
INNER JOIN log.catch_ll c on  s.log_set_id = c.log_set_id
INNER JOIN ref.vessels v ON v.vessel_id = t.vessel_id
INNER JOIN ref.vessel_instances vi ON vi.vessel_id = v.vessel_id and t.depart_date between vi.start_date and vi.calculated_end_date
WHERE eez_code IN ('CK','FJ','MA','NC','PF')
and YEAR(logdate) > 1999
and l_activity_id = 1
group by
t.log_trip_id,
s.log_set_id,
vi.vesselname,
v.vessel_id,
vi.flag_id,
v.loa,
v.grt,
s.latd,
s.lond,
s.in_AWs,
YEAR(logdate),
MONTH(logdate),
DAY(logdate),
set_time,
s.l_activity_id,
eez_code,
hooks_n
order by vi.flag_id, vi.vesselname,	YEAR(logdate),  MONTH(logdate),  DAY(logdate) ")
ls_dat$lond <- ifelse(ls_dat$lond < 0, ls_dat$lond + 360, ls_dat$lond)
save(ls_dat, file = paste0(dir_pth, "Data/OPLL_dat_five_eezs.RData"))
library(MASS)
library(ggplot2)
library(tidyverse)
library(ggthemes)
library(lubridate)
library(magrittr)
library(purrr)
library(randomForest)
library(randomForestExplainer)
library(data.table)
theme_set(theme_bw())
base_pth <- "C:/Albacore_Catch_Modelling/"
save_pth <- "C:/Albacore_Catch_Modelling/Full_process/"
cnt <- "CK"
cnt_low <- "ck"
yr_rng <- 2015:2025
# Read in logsheets filtered to the common LS-VMS fleet
ls_full <- read.csv(file = paste0(save_pth, "Data/", cnt, "/logsheet_full.csv"), header = TRUE) %>% mutate(ves_day = paste(set_day, vessel_id), ent_date = as_date(lub_ent_date))
# Read in logsheets filtered to the common LS-VMS fleet
ls_set <- read.csv(file = paste0(save_pth, "Data/", cnt, "/logsheet_set.csv"), header = TRUE) %>% mutate(ves_day = paste(set_day, vessel_id), ent_date = as_date(lub_ent_date))
# Read in VMS data filtered to the common LS-VMS fleet
vms_set <- read.csv(file = paste0(save_pth, "Data/", cnt, "/vms_set.csv"), header = TRUE) %>% mutate(ves_day = paste(set_day, vessel_id), set_date = as_date(lub_date))
# For the modelling, consolidate the flags down to the main flags operating - put all minor flags in with TW as they are likely more similar operationally
ls_full$flag_id <- ifelse(ls_full$flag_id %in% c("CN","CK","FM"), ls_full$flag_id, "CN") # Yes I could leave TW out of the vector, but for clarity
ls_set$flag_id <- ifelse(ls_set$flag_id %in% c("CN","CK","FM"), ls_set$flag_id, "CN")
vms_set$flag_id <- ifelse(vms_set$flag_id %in% c("CN","CK","FM"), vms_set$flag_id, "CN")
run_CEVT = function(focal_yr = 2024, focal_day = 30, min_ls_obs = 50){
focal_date <- ymd(paste(focal_yr, "1", "1", sep = "-")) + days(focal_day)
# All logsheets up to the focal day in the focal year from the common ls-vms dataset
dat_ls <- ls_set %>% filter(year <= focal_yr, (ent_date - focal_date) <= 0)   # THIS DATE FILTER NEEDS CHECKING
# All logsheets up to the focal day in the focal year from the common ls-vms dataset
dat_ref <- ls_set %>% filter(year <= focal_yr)
# All logsheets up to the focal day in the focal year for the full dataset - to use in modelling
dat_full <- ls_full %>% filter(year < focal_yr | (year == focal_yr & (ent_date - focal_date) <= 0))   # THIS DATE FILTER NEEDS CHECKING
#################################
# Check this date filter above - it should be < 0 at the least, and even that assumes that if entered date is the same date as app is run that it is ready to be used??? same as the line below for VMS
#################################
# All VMS data up to the focal day in the focal year from the common ls-vms dataset
dat_vms <- vms_set %>% filter(year <= focal_yr, (set_date - focal_date) <= 0)   # THIS DATE FILTER NEEDS CHECKING
# Some of the next few lines are redundant given the filtering above...
dat_ls_yr <- dat_ls %>% filter(year == focal_yr) # Note that this will have less logsheets in the focal year than dat_full as any vessels that don't also have vms are removed
ves_dat_ls <- dat_ls_yr %>% group_by(set_day, vesselname) %>% summarise(N = n()) %>% mutate(Type = "observed")
dat_vms_yr <- dat_vms %>% filter(year == focal_yr)
ves_dat_vms <- dat_vms_yr %>% group_by(set_day, vesselname) %>% summarise(N = n()) %>% mutate(Type = "Imputed")
ves_tile_data <- rbind(ves_dat_ls, ves_dat_vms)
if(dim(ves_tile_data)[1] == 0) ves_tile_data <- data.frame(set_day = 1, vesselname = "No fishing yet", N = 1, Type = "Observed")
vms_known <- dat_vms_yr %>% filter(ves_day %in% dat_ls_yr$ves_day)
vms_missing <- dat_vms_yr %>% filter(!ves_day %in% dat_ls_yr$ves_day)
chg_yr <- dim(filter(dat_full, year == focal_yr))[1]
mod_dat <- dat_full %>% select(alb_kg_est, year, month, flag_id, latd, lond, vesselname) %>% mutate(dat_type = "model")
# If this chg_yr value is lower than a threshold, change the year to the last year
if(chg_yr < min_ls_obs){
dat_ls_yr %<>% mutate(year = focal_yr - 1)
dat_vms_yr %<>%  mutate(year = focal_yr - 1)
vms_known %<>%  mutate(year = focal_yr - 1)
vms_missing %<>%  mutate(year = ifelse(year == focal_yr, focal_yr - 1, year))
mod_dat %<>% mutate(year = ifelse(year == focal_yr, focal_yr - 1, year))
}
#################################
# Consider getting rid of all the factors in dataframes, and only make factors in the models themselves
#################################
pred_dat <- vms_missing %>% mutate(alb_kg_est = NA, latd = (min_latitude + max_latitude)/2, lond = (min_longitude + max_longitude)/2, year = factor(year),
month = factor(month(set_date)),
flag_id = ifelse(flag_id %in% unique(mod_dat$flag_id), flag_id, sample(mod_dat$flag_id,length(flag_id))),   # This is to prevent novel flags only encountered in the pred data from crashing predictions
#vessel_id = ifelse(vessel_id %in% unique(mod_dat$vessel_id), vessel_id, sample(mod_dat$vessel_id,length(vessel_id))),
dat_type = "prediction") %>%
#select(alb_kg_est, year, month, flag_id, latd, lond, vessel_id, dat_type)
select(alb_kg_est, year, month, flag_id, latd, lond, vesselname, dat_type)
comb_dat <- rbind(mod_dat, pred_dat) %>% mutate(year = factor(year), month = factor(month), flag_id = factor(flag_id))#, vessel_id = factor(vessel_id))
# These lines, combining, and then splitting etc., are to ensure the factors match between modelling and prediction i.e. the prediction step is carried out correctly
mod_dat <- comb_dat %>% filter(dat_type == "model", !is.na(alb_kg_est)) %>% select(-dat_type)
pred_dat <- comb_dat %>% filter(dat_type == "prediction") %>% select(-dat_type)
#____________________________
# Random forest for predicting missing catches
#  rf1 <- randomForest(alb_kg_est ~ year+month+flag_id+latd+lond+vessel_id, data = mod_dat, ntree = 50, mtry = 3, localImp = TRUE)
rf1 <- randomForest(alb_kg_est ~ year+month+flag_id+latd+lond, data = mod_dat, ntree = 50, mtry = 3, localImp = TRUE)
#print(rf1)
#____________________________
# Delta gamma glm approach for predicting missing catches
zcat_dat <- mod_dat %>% mutate(zcat = ifelse(alb_kg_est == 0, 0, 1))
posdat <- filter(zcat_dat, zcat == 1)
unique(zcat_dat$year); unique(posdat$year)
unique(zcat_dat$month); unique(posdat$month)
unique(zcat_dat$flag_id); unique(posdat$flag_id)
if(length(unique(zcat_dat$year)) > 1){
binmod <- glm(zcat ~ factor(year) + factor(month) + factor(flag_id), data = zcat_dat, family = binomial(link = logit))
} else{
binmod <- glm(zcat ~ factor(month) + factor(flag_id), data = zcat_dat, family = binomial(link = logit))
}
if(length(unique(zcat_dat$year)) > 1){
gammod <- glm(alb_kg_est ~ factor(year) + factor(month) + factor(flag_id), data = posdat, family = Gamma(link = "log"))
covariate_dat <- pred_dat %>% select(year, month, flag_id)
des_matrix <- model.matrix(~covariate_dat$year + covariate_dat$month + covariate_dat$flag_id)
} else{
gammod <- glm(alb_kg_est ~ factor(month) + factor(flag_id), data = posdat, family = Gamma(link = "log"))
covariate_dat <- pred_dat %>% select(month, flag_id)
des_matrix <- model.matrix(~covariate_dat$month + covariate_dat$flag_id)
}
vms_missing %<>% mutate(alb_kg_est = predict(rf1, pred_dat),
binpred = predict.glm(binmod, newdata = pred_dat, type = "response"),
gampred = predict.glm(gammod, newdata = pred_dat, type = "response"),
alb_kg_est_glm = binpred*gampred)
# GLM uncertainty parametric bootstrap
bin_coef <- binmod$coefficients
gam_coef <- gammod$coefficients
gam_disp_param <- summary(gammod)$dispersion
bin_vcov <- vcov(binmod)
gam_vcov <- vcov(gammod)
bin_samp <- mvrnorm(1000, bin_coef, bin_vcov)
gam_samp <- mvrnorm(1000, gam_coef, gam_vcov)
########################
# Should I also include error in the estimate of the dispersion parameter?
# There is a problem when there are some factor levels present in the full binomial data that aren't present in the positive gamma model - need a way to avoid this
########################
samp_models <- function(x){
logit_bin_x <- des_matrix %*% bin_samp[x,]
nom_bin_x <- 1/(1 + exp(-(logit_bin_x)))
nom_bin_x_ran <- rbinom(n = length(nom_bin_x), size = 1, prob = nom_bin_x)
log_gam_x <- des_matrix %*% gam_samp[x,]
nom_gam_x <- exp(log_gam_x)
nom_gam_x_ran <- rgamma(length(nom_gam_x), scale = gam_disp_param*nom_gam_x, shape = 1/gam_disp_param)
DLG_pred <- vms_missing %>% mutate(pred = nom_bin_x_ran*nom_gam_x_ran, samp = x)
return(DLG_pred)
}
sum_df <- map_df(1:1000, samp_models)
#____________________________
cum_ls <- dat_ls_yr %>% group_by(set_day) %>% summarise(Nsets_ls = n(), alb_catch_ls = sum(alb_kg_est))
cum_miss <- vms_missing %>% group_by(set_day) %>% summarise(Nsets_miss = n(), alb_catch_miss = sum(alb_kg_est), alb_catch_miss_glm = sum(alb_kg_est_glm))
cum_ref <- dat_ref %>% filter(year == focal_yr) %>% group_by(set_day) %>% summarise(Nsets_ref = n(), alb_catch_ref = sum(alb_kg_est))
dat_all <- data.frame(day_date = seq(ymd(paste(focal_yr, "1", "1", sep = "-")), ymd(paste(focal_yr, "12", "31", sep = "-")), by = "days")) %>% mutate(set_day = yday(day_date))
dat_all <- left_join(dat_all, cum_ref, by = "set_day")
dat_all <- left_join(dat_all, cum_ls, by = "set_day")
dat_all <- left_join(dat_all, cum_miss, by = "set_day")
dat_all[is.na(dat_all)] <- 0
dat_all %<>% mutate(cum_alb_ref = cumsum(alb_catch_ref), cum_alb_ls = cumsum(alb_catch_ls), cum_alb_miss = cumsum(alb_catch_miss), cum_alb_miss_glm = cumsum(alb_catch_miss_glm))
dat_all_lng <- dat_all %>% select(set_day, observed = cum_alb_ls, imputed = cum_alb_miss, actual = cum_alb_ref, imputed_glm = cum_alb_miss_glm) %>% pivot_longer(cols = -"set_day", names_to = "Type", values_to = "Cum_alb")
pl_ref <- filter(dat_all_lng, Type == "actual")
pl_predict <- filter(dat_all_lng, !Type %in% c("actual","imputed_glm"))
pl_predict$Type <- factor(pl_predict$Type, levels = c("imputed","observed"))
pl_tot <- pl_predict %>% filter(set_day <= focal_day) %>% group_by(set_day) %>% summarise(Cum_alb = sum(Cum_alb))
max_y <- 1.15*max(pl_ref$Cum_alb/1000)
pl_predict_glm <- filter(dat_all_lng, !Type %in% c("actual","imputed")) %>% mutate(Type = recode(Type, "observed" = 'observed logsheets', "imputed_glm" = "missing logsheets"))
pl_predict_glm$Type <- factor(pl_predict_glm$Type, levels = c("missing logsheets", "observed logsheets"))
pl_tot_glm <- pl_predict_glm %>% filter(set_day <= focal_day) %>% group_by(set_day) %>% summarise(Cum_alb = sum(Cum_alb))
pl_current <- pl_tot_glm[dim(pl_tot_glm)[1],]
dat_all_lng %<>% mutate(yr = focal_yr, doy = focal_day)
cum_miss_boot <- sum_df %>% group_by(samp, set_day) %>% summarise(alb_catch_miss_glm = sum(pred))
#################################
# Need to decide on CI width, and change the name of all objects away from "95" unless that value is used
#################################
cum_miss_boot_summary <- cum_miss_boot %>% group_by(set_day) %>% summarise(LL95 = quantile(alb_catch_miss_glm, 0.1), UL95 = quantile(alb_catch_miss_glm, 0.9))
boot_all <- data.frame(day_date = seq(ymd(paste(focal_yr, "1", "1", sep = "-")), ymd(paste(focal_yr, "12", "31", sep = "-")), by = "days")) %>% mutate(set_day = yday(day_date))
boot_all <- left_join(boot_all, cum_miss_boot_summary, by = "set_day")
boot_all[is.na(boot_all)] <- 0
ref_dat <- dat_all %>% select(set_day, alb_catch_ls)
boot_all <- left_join(ref_dat, boot_all, by = "set_day")
boot_all_sht <- boot_all %>% mutate(LL95_tot = alb_catch_ls + LL95, UL95_tot = alb_catch_ls + UL95,
cum_LL95 = cumsum(LL95_tot), cum_UL95 = cumsum(UL95_tot)) %>% filter(set_day <= focal_day)
#################################
# Check why estimates go to set day - focal_day + 1
#################################
windows(3000,2000)
pl = ggplot() + geom_line(data = pl_ref, aes(x = set_day, y = Cum_alb/1000), linewidth = 1.2) +
geom_bar(data = pl_predict_glm, aes(x = set_day, y = Cum_alb/1000, fill = Type), stat = "identity", width = 1) +
geom_vline(xintercept = focal_day, colour = alpha("dodgerblue", .5)) +
geom_hline(yintercept = pl_current$Cum_alb/1000, colour = alpha("dodgerblue", .5), linetype = 2) +
geom_ribbon(data = boot_all_sht, aes(x = set_day, ymin = cum_LL95/1000, ymax = cum_UL95/1000), fill = alpha("red", .15), colour = "grey30") + #, linetype = 2) +
geom_line(data = pl_tot, aes(x = set_day, y = Cum_alb/1000), linewidth = 1, colour = alpha("green", .7)) +
geom_line(data = pl_tot_glm, aes(x = set_day, y = Cum_alb/1000), linewidth = 1.2, colour = alpha("red", .9)) +
geom_point(data = pl_current, aes(x = set_day, y = Cum_alb/1000), colour = alpha("red", .99), size = 5) +
annotate("text", x = 0, y = 1.05*pl_current$Cum_alb/1000, label = paste(round(pl_current$Cum_alb/1000), "mt"), colour = "dodgerblue", fontface = 2, size = 5) +
annotate("text", x = 40, y = .95*max_y, label = boot_all$day_date[focal_day], size = 7, colour = "grey30") +
annotate("text", x = 40, y = .88*max_y, label = paste("Day", focal_day), size = 7, colour = "grey30") +
xlab("Day of year") + ylab("Cumulative catch of albacore (mt)") +
scale_fill_manual(values = c(alpha("navy", .2), alpha("navy", .7))) + theme_clean() +
scale_x_continuous(breaks = seq(1, 365, 40), labels = boot_all$day_date[seq(1, 365, 40)]) +
scale_y_continuous(limits = c(NA, max_y), breaks = seq(0, max_y, 250)) +
theme(axis.title = element_text(size = 16), axis.text.x = element_text(size = 14, angle = 45, vjust = 0.5, hjust = .5),
axis.text.y = element_text(size = 14), legend.position = "top",
legend.background = element_blank(), legend.title = element_blank())
print(pl)
savePlot(filename = paste0(save_pth, "Plots/CK/CEVT_", focal_yr, "_", focal_day, ".png"), type = "png")
dev.off()
cum_ls_ves <- dat_ls_yr %>% group_by(set_day, vesselname_normalised) %>%
summarise(Nsets_ref = n(), alb_catch_ref = sum(alb_kg_est)) %>% group_by(Vessel = vesselname_normalised) %>%
summarise(Nsets_ref = n(), alb_catch_ref = sum(alb_catch_ref)) %>% mutate(Type = "observed")
cum_miss_ves <- vms_missing %>% group_by(set_day, vesselname_normalised) %>%
summarise(Nsets_ref = n(), alb_catch_ref = sum(alb_kg_est_glm)) %>% group_by(Vessel = vesselname_normalised) %>%
summarise(Nsets_ref = n(), alb_catch_ref = sum(alb_catch_ref)) %>% mutate(Type = "imputed")
cum_ref_ves <- dat_ref %>% filter(year == focal_yr, set_day <= focal_day) %>% group_by(set_day, vesselname_normalised) %>%
summarise(Nsets_ref = n(), alb_catch_ref = sum(alb_kg_est)) %>% group_by(Vessel = vesselname_normalised) %>%
summarise(Nsets_ref = n(), alb_catch_ref = sum(alb_catch_ref))
pl_ves <- rbind(cum_ls_ves, cum_miss_ves)
ves_order <- pl_ves %>% group_by(Vessel) %>% summarise(Total = sum(alb_catch_ref)) %>% arrange(Total)
pl_ves$Vessel <- factor(pl_ves$Vessel, levels = ves_order$Vessel)
windows(2000,2000)
pl = ggplot() + geom_bar(data = pl_ves, aes(x = Vessel, y = alb_catch_ref/1000, fill = Type), stat = "identity", width = .8) +#, colour = "black") +
scale_fill_manual(values = c(alpha("navy", .1), alpha("navy", .5))) +
geom_bar(data = cum_ref_ves, aes(x = Vessel, y = alb_catch_ref/1000), stat = "identity", width = .8, fill = alpha("white",.01), colour = alpha("black", .3)) +
coord_flip() + xlab("Vessel") + ylab("Cumulative catch of albacore (mt)") + theme_clean() +
theme(axis.title = element_text(size = 16), axis.text = element_text(size = 14), legend.position = "top",
legend.background = element_blank(), legend.title = element_blank())
print(pl)
dev.off()
#_______________________________________________________________________________
# Forecasting module
day_dat <- ls_set %>% group_by(lub_set_date, vessel_id) %>% summarise(alb_kg_est = sum(alb_kg_est, na.rm = TRUE), N = n()) %>%
group_by(lub_set_date) %>% summarise(alb_kg_est = sum(alb_kg_est), Nset = sum(N, na.rm = TRUE)) %>%    # On a couple of occasions this will overestimate vessels
arrange(lub_set_date) %>% mutate(Year = year(lub_set_date), Month = month(lub_set_date)) %>%           # e.g. occasionally there will be 2 sets on a day by a single vessel
ungroup() %>% mutate(lub_set_date = ymd(lub_set_date))
cum_day_dat <- day_dat %>% group_by(Year) %>% mutate(cum_alb = cumsum(alb_kg_est))
dat_frc <- data.frame(lub_set_date = seq(ymd(paste(min(yr_rng), "1", "1", sep = "-")), ymd(paste(max(yr_rng), "12", "31", sep = "-")), by = "days"))
dat_frc <- left_join(dat_frc, day_dat, by = "lub_set_date") %>% mutate(Year = year(lub_set_date), Month = month(lub_set_date), Day = yday(lub_set_date))
dat_frc[is.na(dat_frc)] <- 0
# For the model fitting dataset, remove all data from the focal year
# The "data" used in the focal_year comes from the CEVT prediction above (dat_all) for the focal year - this essentially gives the year effect
dat_mod <- dat_frc %>% filter(!(Year == focal_yr)) %>% #mutate(Nset_orig = Nset, predalb = alb_kg_est)
select(Year, Month, Day, Nset, alb_kg_est)
dat_all_both <- dat_all %>% mutate(alb_kg_est = alb_catch_ls + alb_catch_miss_glm,
Year = year(day_date), Month = month(day_date), Day = yday(day_date),
Nset = Nsets_ls + Nsets_miss) %>% # Need to check that this Nset calc is the correct one, not Nsets_ref
select(Year, Month, Day, Nset, alb_kg_est)
if(focal_day < 30) dat_all_both$Year <- dat_all_both$Year - 1
dat_all_current <- filter(dat_all_both, Day <= focal_day)
dat_all_future <- filter(dat_all_both, Day > focal_day) %>% mutate(Nset = mean(dat_all_current$Nset)) # Give each future day the mean number of sets observed during focal year (in future include the uncertainty in this)
# Note that this modelling approach uses data from all years, not just those prior to the focal year. This is more akin to a leave one out cross validation, not a sequential model
# like the historical modelling module. This was because we need multiple years of data to get decent estimates i.e. it would not work well for 2016, 17, 18 etc.
dat_mod <- rbind(dat_mod, dat_all_current)
# The next set of code with the if statement is to prevent the latest (incomplete) year of data being used in calibrating the model
if(focal_yr == max(yr_rng)){
dat_mod %<>% filter(!(Year == max(yr_rng) & Day > focal_day))
} else {
dat_mod %<>% filter(Year != max(yr_rng))
}
########################
# Note that currently using log normal GLM - this might be more suitable for other EEZs, need to revisit later to improve...
########################
dat_comb <- rbind(dat_mod, dat_all_future)   # Need to combine the modelling data and the "future" (prediction) data to ensure the design matrix for predictions is the correct dimensions
kg_constant = 15 # Set the constant at 15kgs now (the lowest observed non-zero value)
dat_mod %<>% mutate(log_alb_kg_est = log(alb_kg_est + kg_constant))
dat_mod$wk <- ceiling(dat_mod$Day/7)
if(length(unique(dat_mod$year)) > 1){
lnmod <- glm(log_alb_kg_est ~ Nset + factor(Year) + factor(Month), data = dat_mod)
des_matrix <- model.matrix(~dat_comb$Nset + factor(dat_comb$Year) + factor(dat_comb$Month))
des_matrix <- des_matrix[(nrow(dat_mod) + 1):nrow(dat_comb),]
formod  <- glm(log_alb_kg_est ~ factor(Year) + factor(wk), data = dat_mod)
} else{
lnmod <- glm(log_alb_kg_est ~ Nset + factor(Month), data = dat_mod)
des_matrix <- model.matrix(~dat_comb$Nset + factor(dat_comb$Month))
des_matrix <- des_matrix[(nrow(dat_mod) + 1):nrow(dat_comb),]
formod  <- glm(log_alb_kg_est ~ factor(wk), data = dat_mod)
}
LN_coef <- lnmod$coefficients
LN_disp_param <- summary(lnmod)$dispersion
LN_vcov <- vcov(lnmod)
des_matrix_mod <- model.matrix(lnmod)
manual_fitted <- des_matrix_mod %*% LN_coef # Can test this against fitted(lnmod) - should be equivalent
LN_samp <- mvrnorm(1000, LN_coef, LN_vcov)
samp_models_LN <- function(x){
log_x <- des_matrix %*% LN_samp[x,]
log_x_ran <- rnorm(length(log_x), mean = log_x, sd = sqrt(LN_disp_param))
# Need to add the log-normal error here before backtransforming in the next year
exp_x <- exp(log_x_ran) - kg_constant
LN_pred <- dat_all_future %>% mutate(pred = exp_x, samp = x)
return(LN_pred)
}
sum_df_LN <- map_df(1:1000, samp_models_LN)
#############################
# Need to check why there are catches for focal_day + 1 in dat_all_future
#############################
####################
# If we add other covariates then need to make sure the prediction and the model have consistent factor levels - just year and month will be fine
###################
dat_all_future$alb_kg_est <- exp(predict(lnmod, newdata = dat_all_future)) - kg_constant
dat_all_future$wk <- ceiling(dat_all_future$Day/7)
dat_all_future$alb_kg_est_for <- exp(predict(formod, newdata = dat_all_future)) - kg_constant
####################
# Note that I think the prediction above is the median? Should prob change to mean by using var from model and log-normal bia correction
###################
dat_all_current %<>% mutate(Cum_alb = cumsum(alb_kg_est))
dat_all_future$alb_kg_est[1] <- dat_all_future$alb_kg_est[1] + last(dat_all_current$Cum_alb) # Need to make sure 1st future value includes cumulative catch up to the prior day
dat_all_future$alb_kg_est_for[1] <- dat_all_future$alb_kg_est_for[1] + last(dat_all_current$Cum_alb) # Need to make sure 1st future value includes cumulative catch up to the prior day
dat_all_future %<>% mutate(Cum_alb = cumsum(alb_kg_est), Cum_alb_for = cumsum(alb_kg_est_for))
boot_limits <- sum_df_LN %>% group_by(Day) %>% summarise(LL95 = quantile(pred, 0.1), UL95 = quantile(pred, 0.9))
dat_all_future <- left_join(dat_all_future, boot_limits, by = "Day")
#########################
# Need to incorporate the LL and UL from the historical estimates into these projection CIs
#########################
dat_all_future$LL95[1] <- dat_all_future$LL95[1] + last(boot_all_sht$cum_LL95)
dat_all_future$UL95[1] <- dat_all_future$UL95[1] + last(boot_all_sht$cum_UL95)
dat_all_future %<>% mutate(cum_LL95 = cumsum(LL95), cum_UL95 = cumsum(UL95))
pl_current <- pl_tot_glm[dim(pl_tot_glm)[1],]
max_y <- 1.35*max(pl_ref$Cum_alb/1000) #max(1.35*max(pl_ref$Cum_alb/1000), dat_all_future$Cum_alb/1000*1.1)
dat_all_future_tmp <- dat_all_future
dat_all_future_tmp$cum_UL95 <- ifelse(dat_all_future$cum_UL95 > max_y*1000, max_y*1000, dat_all_future$cum_UL95)
windows(3000,2000)
pl = ggplot() + geom_line(data = pl_ref, aes(x = set_day, y = Cum_alb/1000), linewidth = 1.2) +
geom_bar(data = pl_predict_glm, aes(x = set_day, y = Cum_alb/1000, fill = Type), stat = "identity", width = 1) +
geom_vline(xintercept = focal_day, colour = alpha("dodgerblue", .5)) +
geom_hline(yintercept = pl_current$Cum_alb/1000, colour = alpha("dodgerblue", .5), linetype = 2) +
geom_ribbon(data = boot_all_sht, aes(x = set_day, ymin = cum_LL95/1000, ymax = cum_UL95/1000), fill = alpha("red", .15), colour = "grey30") + #, linetype = 2) +
geom_line(data = pl_tot, aes(x = set_day, y = Cum_alb/1000), linewidth = 1, colour = alpha("green", .7)) +
geom_line(data = pl_tot_glm, aes(x = set_day, y = Cum_alb/1000), linewidth = 1.2, colour = alpha("red", .9)) +
geom_point(data = pl_current, aes(x = set_day, y = Cum_alb/1000), colour = alpha("red", .99), size = 5) +
geom_line(data = dat_all_future_tmp, aes(x = Day, y = Cum_alb/1000), linewidth = .9, colour = alpha("deeppink", .3)) +
geom_line(data = dat_all_future_tmp, aes(x = Day, y = Cum_alb_for/1000), linewidth = .9, colour = alpha("grey", .3)) +
geom_ribbon(data = dat_all_future_tmp, aes(x = Day, ymin = cum_LL95/1000, ymax = cum_UL95/1000), fill = alpha("yellow", .15)) + #, linetype = 2) +
geom_point(data = pl_current, aes(x = set_day, y = Cum_alb/1000), colour = alpha("red", .99), size = 5) +
annotate("text", x = 0, y = 1.05*pl_current$Cum_alb/1000, label = paste(round(pl_current$Cum_alb/1000), "mt"), colour = "dodgerblue", fontface = 2, size = 5) +
annotate("text", x = 40, y = .95*max_y, label = boot_all$day_date[focal_day], size = 7, colour = "grey30") +
annotate("text", x = 40, y = .88*max_y, label = paste("Day", focal_day), size = 7, colour = "grey30") +
xlab("Day of year") + ylab("Cumulative catch of albacore (mt)") +
scale_fill_manual(values = c(alpha("navy", .2), alpha("navy", .7))) + theme_clean() +
scale_x_continuous(breaks = seq(1, 365, 40), labels = boot_all$day_date[seq(1, 365, 40)]) +
scale_y_continuous(limits = c(NA, max_y), breaks = seq(0, max_y, 250)) +
theme(axis.title = element_text(size = 16), axis.text.x = element_text(size = 14, angle = 45, vjust = 0.5, hjust = .5),
axis.text.y = element_text(size = 14), legend.position = "top",
legend.background = element_blank(), legend.title = element_blank())
print(pl)
savePlot(filename = paste0(save_pth, "Plots/", cnt, "/CEVT2_", focal_yr, "_", focal_day, ".png"), type = "png")
dev.off()
#_______________________________________________________________________________
return(list(ves_tile_data = ves_tile_data, pl_ref = pl_ref, pl_tot = pl_tot, pl_tot_glm = pl_tot_glm, dat_all_lng = dat_all_lng,
dat_all_future = dat_all_future, pl_predict = pl_predict, pl_predict_glm = pl_predict_glm,
pl_ves = pl_ves, pl_current = pl_current, cum_ref_ves = cum_ref_ves, boot_all = boot_all,
boot_all_sht = boot_all_sht, focal_yr = focal_yr, focal_day = focal_day, min_ls_obs = min_ls_obs))
}
#_______________________________________________________________________________
# Run CEVT sequentially
# Example for 1 time step:
# run_CEVT(focal_yr = 2016, focal_day = 80, min_ls_obs = 50)
param_df <- expand.grid(x = 2016:2024, y = seq(10,360,20)) %>% arrange(x, y)
write.csv(param_df, file = paste0(save_pth, "Output/CK/CEVT_control_file.csv"), row.names = FALSE)
CEVT_output <- map2(.x = param_df$x, .y = param_df$y, .f = run_CEVT, min_ls_obs = 50)
save(CEVT_output, file = paste0(save_pth, "Output/CK/CEVT_Results_Saved.RData"))
# Run all periods for the current year
#param_df <- expand.grid(x = 2024, y = seq(10,360,5)) %>% arrange(x, y)
cur_dt <- ymd(Sys.Date())
param_df <- data.frame(x = year(cur_dt), y = yday(cur_dt))
param_df
write.csv(param_df, file = paste0(save_pth, "Output/CK/CEVT_control_file_Latest.csv"), row.names = FALSE)
CEVT_output_latest <- map2(.x = param_df$x, .y = param_df$y, .f = run_CEVT, min_ls_obs = 50)
save(CEVT_output_latest, file = paste0(save_pth, "Output/CK/CEVT_Results_Saved_Latest.RData"))
runApp('C:/GitRep/CEVT_CK')
runApp('C:/GitRep/CEVT_CK')
shiny::runApp()
runApp()
758/10267
758/10267*9803
758+447+2465
runApp()
runApp()
shiny::runApp()
runApp('C:/GitRep/CEVT_CK')
